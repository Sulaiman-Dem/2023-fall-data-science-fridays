{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the TFIDF [~10min]\n",
    "Watch the StatQuest Word Embeddings video [~20min]\n",
    " UPDATE:  Find and link 2 tutorials that will help you do your project.\n",
    "As always, respond to the slack message with one thing you learned that has not been mentioned yet.\n",
    "There will be a short quiz on the pre-class material.\n",
    "YouTubeYouTube | ritvikmath\n",
    "TFIDF : Data Science Concepts \n",
    "YouTubeYouTube | StatQuest with Josh Starmer\n",
    "Word Embedding and Word2Vec, Clearly Explained!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.youtube.com/watch?v=ATK6fm3cYfI more help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF = Understanding the data at hand.\n",
    "TFIDF is a multiplation of two matrix which "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TF = Term - Frequency\n",
    "2. IDF = Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "(f(t, d)) = \n",
    "number occurrence of t in d / number total words in d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. \n",
    "idf (t, D) = log (N/ number docs w/t) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = number of documents\n",
    "D = corpus which is everything \n",
    "w/t = contain the number of terms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \n",
    "tf(t,D) x idf (t,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding and Word2Vec, Clearly Explained!!!\n",
    "1. https://www.youtube.com/watch?v=viZrOnJclY0&amp;ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec: \n",
    "Having similar words with similar embeddings means training a neural network to process language easier \n",
    "> 2. Continuous Bag of words = Increases the context by using the surrounding words to predict what occurs in the middle\n",
    "> 3. Skip Gram = Increases the context by using the word in the middle to predict the surrounding words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For efficiency, Negative Sampling works by randomly selecting a subset of words we don't want to predict for optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
